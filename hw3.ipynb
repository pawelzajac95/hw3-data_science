{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 74070773872.67436\n",
      "Current Parameters: [4.71081795e-03 4.69273815e+03 3.87846675e+03 3.87847435e+03]\n",
      "Iteration 1000: Cost = 70190402694.72072\n",
      "Current Parameters: [ 4.71081795e-03  1.71610168e+05 -4.15831751e+04 -4.15831675e+04]\n",
      "Iteration 2000: Cost = 70190099886.36334\n",
      "Current Parameters: [ 4.71081795e-03  1.73791969e+05 -4.26500526e+04 -4.26500450e+04]\n",
      "Iteration 3000: Cost = 70190099815.86847\n",
      "Current Parameters: [ 4.71081795e-03  1.73825258e+05 -4.26663309e+04 -4.26663233e+04]\n",
      "Iteration 4000: Cost = 70190099815.85205\n",
      "Current Parameters: [ 4.71081795e-03  1.73825766e+05 -4.26665793e+04 -4.26665717e+04]\n",
      "Iteration 5000: Cost = 70190099815.85205\n",
      "Current Parameters: [ 4.71081795e-03  1.73825774e+05 -4.26665831e+04 -4.26665755e+04]\n",
      "Iteration 6000: Cost = 70190099815.85205\n",
      "Current Parameters: [ 4.71081795e-03  1.73825774e+05 -4.26665831e+04 -4.26665755e+04]\n",
      "Iteration 7000: Cost = 70190099815.85205\n",
      "Current Parameters: [ 4.71081795e-03  1.73825774e+05 -4.26665831e+04 -4.26665755e+04]\n",
      "Iteration 8000: Cost = 70190099815.85205\n",
      "Current Parameters: [ 4.71081795e-03  1.73825774e+05 -4.26665831e+04 -4.26665755e+04]\n",
      "Iteration 9000: Cost = 70190099815.85205\n",
      "Current Parameters: [ 4.71081795e-03  1.73825774e+05 -4.26665831e+04 -4.26665755e+04]\n",
      "Gradient Descent Parameters: [ 4.71081795e-03  1.73825774e+05 -4.26665831e+04 -4.26665755e+04]\n",
      "Analytical Solution Parameters: [     0.         173822.26135593 -42664.89070077 -42664.89069911]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Hypothesis function in vectorized form\n",
    "def hypothesis(X, theta):\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "# 2. Loss function (mean squared error) in vectorized form\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    prediction = hypothesis(X, theta)\n",
    "    cost = (1 / (2 * m)) * np.sum((prediction - y) ** 2)\n",
    "    return cost\n",
    "\n",
    "# 3. Gradient descent step\n",
    "def gradient_step(X, y, theta, learning_rate):\n",
    "    m = len(y)\n",
    "    prediction = hypothesis(X, theta)\n",
    "    gradient = (1 / m) * np.dot(X.T, (prediction - y))\n",
    "    theta = theta - learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "# 4. Finding optimal parameters using gradient descent\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    for i in range(iterations):\n",
    "        theta = gradient_step(X, y, theta, learning_rate)\n",
    "        if i % 1000 == 0: \n",
    "            print(f\"Iteration {i}: Cost = {compute_cost(X, y, theta)}\")\n",
    "            print(f\"Current Parameters: {theta}\")\n",
    "    return theta\n",
    "\n",
    "# 5. Analytical solution with regularization (Ridge Regression)\n",
    "def analytical_solution(X, y, alpha=1e-5):\n",
    "    n = X.shape[1]\n",
    "    I = np.eye(n) \n",
    "    return np.linalg.inv(X.T @ X + alpha * I) @ X.T @ y\n",
    "\n",
    "# 6. Normalize data with protection against division by zero\n",
    "def normalize(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std_dev = np.std(X, axis=0)\n",
    "    std_dev[std_dev == 0] = 1 \n",
    "    return (X - mean) / std_dev\n",
    "\n",
    "# 7. Compare results from both methods\n",
    "def compare_results(X, y):\n",
    "    # Normalize the feature matrix\n",
    "    X = normalize(X)\n",
    "\n",
    "    # Initializing parameters (with bias term)\n",
    "    theta_init = np.random.randn(X.shape[1]) * 0.01  # Random small values for initialization\n",
    "\n",
    "    # Finding parameters using gradient descent\n",
    "    theta_gd = gradient_descent(X, y, theta_init, learning_rate=0.05, iterations=10000)\n",
    "\n",
    "    # Finding parameters using the analytical solution\n",
    "    theta_analytical = analytical_solution(X, y)\n",
    "\n",
    "    print(\"Gradient Descent Parameters:\", theta_gd)\n",
    "    print(\"Analytical Solution Parameters:\", theta_analytical)\n",
    "\n",
    "# Sample data: area (sq ft), number of bathrooms, number of bedrooms\n",
    "X = np.array([[2100, 3, 4],\n",
    "              [1600, 2, 3],\n",
    "              [2400, 4, 5],\n",
    "              [1416, 2, 3],\n",
    "              [3000, 4, 5]])\n",
    "\n",
    "# Add a column of ones for the intercept term (bias)\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# House prices (target values)\n",
    "y = np.array([400000, 330000, 369000, 232000, 539900])\n",
    "\n",
    "# Compare the results of both methods\n",
    "compare_results(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
